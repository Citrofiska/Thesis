{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documented-tattoo",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supreme-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-chicken",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-aquatic",
   "metadata": {},
   "source": [
    "Model design made following Albadawi 2020, and open source github repos:\n",
    "- https://github.com/liusongxiang/StarGAN-Voice-Conversion/blob/master/model.py\n",
    "- https://github.com/pritishyuvraj/Voice-Conversion-GAN/blob/master/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-ozone",
   "metadata": {},
   "source": [
    "Creating the residual block for the encoder. Instance norm 2d normalises samples with respect to them self rather than all neighbouring samples in a batch (as is done in batch normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "referenced-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=4, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=4, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-pottery",
   "metadata": {},
   "source": [
    "The rational for the encoder is to rid of all the non-timbral related information in its dimensionality reduction to the latent space. The generator then builds up the new voice from the rich latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coral-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "         \n",
    "        self.l = []\n",
    "        \n",
    "        # Initial linear convolutional mapping\n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=7, bias=False), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        # Non-linear mapping convolutional layers\n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, bias=False, stride=2), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "                 \n",
    "        # Residual blocks with skip connections for bottleneck\n",
    "        self.l.append(ResidualBlock(512, 1024))\n",
    "        self.l.append(ResidualBlock(1024, 1156))\n",
    "        self.l.append(ResidualBlock(1156, 1280))                  \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.l:\n",
    "            x = layer(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-company",
   "metadata": {},
   "source": [
    "Following Albadawi 2020, making the generator the reverse of the encoder for upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "horizontal-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.l = []\n",
    "                \n",
    "        # Residual blocks for expanding from latent space\n",
    "        self.l.append(ResidualBlock(1280, 1156))\n",
    "        self.l.append(ResidualBlock(1156, 1024))\n",
    "        self.l.append(ResidualBlock(1024, 512)) \n",
    "        \n",
    "        # Non-linear mapping convolutional layers\n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, bias=False, stride=2), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        # Final linear convolutional mapping \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=7, bias=False), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.l: \n",
    "            x = layer(x)\n",
    "        return x               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dependent-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dicriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.l = []\n",
    "        \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "        \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(1024, 1280, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "                \n",
    "        self.l.append(nn.Sequential(\n",
    "            nn.Conv2d(1280, 1, kernel_size=4, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.LeakyReLU(0.2)))\n",
    "         \n",
    "    def forward(self, x):\n",
    "        for layer in self.l: \n",
    "            x = layer(x)\n",
    "        return x          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-knock",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

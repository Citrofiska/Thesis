{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documented-tattoo",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supreme-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from utility import load_pickle, save_pickle, show_mel\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-static",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-plaintiff",
   "metadata": {},
   "source": [
    "Model design and training made following Albadawi 2020, and some open source github repos and tutorials:\n",
    "- https://github.com/liusongxiang/StarGAN-Voice-Conversion/blob/master/model.py\n",
    "- https://github.com/pritishyuvraj/Voice-Conversion-GAN/blob/master/model.py\n",
    "- https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "- https://github.com/Lornatang/CycleGAN-PyTorch/blob/master/cyclegan_pytorch/models.py\n",
    "- https://github.com/seangal/dcgan_vae_pytorch/blob/master/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-chicken",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-suffering",
   "metadata": {},
   "source": [
    "Creating the residual block for the encoder. Instance norm 2d normalises samples with respect to them self rather than all neighbouring samples in a batch (as is done in batch normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "referenced-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim_in):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(2),\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size=4, bias=False),\n",
    "            nn.InstanceNorm2d(dim_in),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim_in, dim_in, kernel_size=4, bias=False),\n",
    "            nn.InstanceNorm2d(dim_in))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-adult",
   "metadata": {},
   "source": [
    "The rational for the encoder is to rid of all the non-timbral related information in its dimensionality reduction to the latent space. The generator then builds up the new voice from the rich latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coral-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Initial linear convolutional mapping\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=7, bias=False), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        # Non-linear mapping convolutional layers\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, bias=False, stride=2), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.conv3 =nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.conv4 =nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2)) \n",
    "                 \n",
    "        # Residual blocks with skip connections for bottleneck\n",
    "        self.res5 = ResidualBlock(1024)\n",
    "        self.mu6 = ResidualBlock(1024)\n",
    "        self.logvar6 = ResidualBlock(1024)\n",
    "        \n",
    "        # OLD: Starting with less residual blocks due to memory\n",
    "        # l.append(ResidualBlock(512, 1024))\n",
    "        # l.append(ResidualBlock(1024, 1156))\n",
    "        # l.append(ResidualBlock(1156, 1280)) \n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "    def forward(self, x):     \n",
    "        \n",
    "        # Main layers\n",
    "        x = self.conv1(x)      \n",
    "        x = self.conv2(x)      \n",
    "        x = self.conv3(x)        \n",
    "        x = self.conv4(x)     \n",
    "        x = self.res5(x)     \n",
    "        \n",
    "        # Bottleneck\n",
    "        mu = self.mu6(x)\n",
    "        logvar = self.logvar6(x)   \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-buffalo",
   "metadata": {},
   "source": [
    "Following Albadawi 2020, making the generator the reverse of the encoder for upsampling in a decoder-like manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "horizontal-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.res1 = ResidualBlock(1024)  \n",
    "        \n",
    "        # OLD: Starting with less residual blocks due to memory\n",
    "        # l.append(ResidualBlock(1156, 1024))\n",
    "        # l.append(ResidualBlock(1024, 512)) \n",
    "        \n",
    "        # Non-linear mapping convolutional layers\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=4, bias=False, stride=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, bias=False, stride=2), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, bias=False, stride=2), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2))\n",
    "        \n",
    "        # Final linear convolutional mapping \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=11, bias=False), \n",
    "            nn.Tanh())  # wrt DCGAN\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.res1(x)  \n",
    "        x = self.conv2(x) \n",
    "        x = self.conv3(x) \n",
    "        x = self.conv4(x) \n",
    "        x = self.conv5(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-ecuador",
   "metadata": {},
   "source": [
    "Discriminator is downsampling to a feature space for classification. Followed DCGAN intuition. Strided convolutions with ReLU based activations, and non strided convolution for the last fifth layer with a Sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "plain-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, bias=False, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, bias=False, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, bias=False, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=4, bias=False, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "                \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=8, bias=False),\n",
    "            nn.Sigmoid())  # wrt DCGAN\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-reminder",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-consortium",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "august-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training control\n",
    "max_epochs = 1000\n",
    "max_duplets = 3000\n",
    "batch_size = 4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "assert max_duplets % batch_size == 0, 'Max sample pairs must be divisible by batch size!' \n",
    "\n",
    "# Regularisation\n",
    "lambda_cycle = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-campbell",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-alfred",
   "metadata": {},
   "source": [
    "Loading data and shuffling it and converting to torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "southeast-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "melset_7_128 = load_pickle('pool/melset_7_128.pickle')\n",
    "melset_4_128 = load_pickle('pool/melset_4_128.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reported-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160 3386\n"
     ]
    }
   ],
   "source": [
    "print(len(melset_7_128), len(melset_4_128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "challenging-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "# Shuffling melspectrograms\n",
    "melset_7_128 = rng.permutation(np.array(melset_7_128))\n",
    "melset_4_128 = rng.permutation(np.array(melset_4_128))\n",
    "\n",
    "# Torch conversion\n",
    "melset_7_128 = torch.from_numpy(melset_7_128)\n",
    "melset_4_128 = torch.from_numpy(melset_4_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-commons",
   "metadata": {},
   "source": [
    "### Model Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-buffalo",
   "metadata": {},
   "source": [
    "Following Albadawi 2020, unlike the other models, there is only one universal encoder in attempt to train it to rid of frequency and loudness information across speakers.\n",
    "\n",
    "The first residual block of each generator was common in Albadawi 2020. For simplicity, the first ecoding residual block is not individual to each decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "placed-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory if models previously initialised\n",
    "import gc\n",
    "if 'E' in globals(): del E \n",
    "if 'G_A2B' in globals(): del G_A2B\n",
    "if 'D_B' in globals(): del D_B \n",
    "if 'G_B2A' in globals(): del G_B2A\n",
    "if 'D_A' in globals(): del D_A\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "# Shared encoder model (with partial decoding)\n",
    "E = Encoder().to(device)\n",
    "\n",
    "# Generator and Discriminator for Speaker A to B\n",
    "G_A2B = Generator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "# Generator and Discriminator for Speaker B to A\n",
    "G_B2A = Generator().to(device)\n",
    "D_A = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-bacon",
   "metadata": {},
   "source": [
    "### Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-account",
   "metadata": {},
   "source": [
    "Initialise weights from dist with mu=0, s=0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adjustable-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afraid-reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E.apply(weights_init)\n",
    "G_A2B.apply(weights_init)\n",
    "G_B2A.apply(weights_init)\n",
    "D_A.apply(weights_init)\n",
    "D_B.apply(weights_init)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-chase",
   "metadata": {},
   "source": [
    "Keeping an experience roleplay buffer for training discriminator on previous iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "seeing-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "quiet-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-charlotte",
   "metadata": {},
   "source": [
    "### Objective Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-search",
   "metadata": {},
   "source": [
    "MSE instead of BCE for adversarial loss making it a Least Squares GAN. Aims to minimize vanishing gradients by making it less discrete, done following CycleGAN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chinese-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising optimizers\n",
    "optim_E = torch.optim.Adam(E.parameters(), lr=learning_rate)\n",
    "optim_G = torch.optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()),lr=learning_rate)\n",
    "optim_D_A = torch.optim.Adam(D_A.parameters(), lr=learning_rate)\n",
    "optim_D_B = torch.optim.Adam(D_B.parameters(), lr=learning_rate)\n",
    "\n",
    "# Inializing criterions\n",
    "loss_adversarial = torch.nn.MSELoss().to(device)\n",
    "loss_cycle = torch.nn.L1Loss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-verse",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-vegetable",
   "metadata": {},
   "source": [
    "Training loop implemented based on Albadawi 2020 for theory and CycleGAN for implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "labeled-electronics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf48e40f1d354520a0a49c6ed8eaf60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=1000.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ba51bb2c748108149edf611c72c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=750.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/rs002/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-20-b5fa20ff6289>\", line 90, in <module>\n",
      "    errG.backward()\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/torch/tensor.py\", line 195, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 97, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/homes/rs002/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/import/linux/python/3.8.2/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/import/linux/python/3.8.2/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/import/linux/python/3.8.2/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/import/linux/python/3.8.2/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b5fa20ff6289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_GAN_A2B\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_GAN_B2A\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cycle_ABA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_cycle_BAB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0moptim_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x144 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(max_epochs), desc='Epochs')  # init epoch pbar\n",
    "\n",
    "# Initialising loss history lists\n",
    "train_hist = {}\n",
    "train_hist['G_B2A'] = []\n",
    "train_hist['G_ABA'] = []\n",
    "train_hist['G_A2B'] = [] \n",
    "train_hist['G_BAB'] = [] \n",
    "train_hist['G'] = []\n",
    "train_hist['D_A'] = []\n",
    "train_hist['D_B'] = []\n",
    "train_hist['VAE_A'] = []\n",
    "train_hist['VAE_B'] = []\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "\n",
    "# Training loop\n",
    "for i in pbar:\n",
    "    \n",
    "    pbar_sub = tqdm(range(0, max_duplets, batch_size),leave=False, desc='Batches')  # init batch pbar \n",
    "    \n",
    "    for j in pbar_sub:\n",
    "        \n",
    "        # Loading real samples from each speaker in batches\n",
    "        real_mel_A = melset_7_128[j : j + batch_size].to(device)\n",
    "        real_mel_B = melset_4_128[j : j + batch_size].to(device)\n",
    "        \n",
    "        # Resizing to model tensors\n",
    "        real_mel_A = real_mel_A.view(batch_size, 1, 128, 128)\n",
    "        real_mel_B = real_mel_B.view(batch_size, 1, 128, 128)\n",
    "\n",
    "        # Real data labelled 1, fake data labelled 0\n",
    "        batch_size = real_mel_A.size(0)\n",
    "        real_label = torch.squeeze(torch.full((batch_size, 1), 1, device=device, dtype=torch.float32))\n",
    "        fake_label = torch.squeeze(torch.full((batch_size, 1), 0, device=device, dtype=torch.float32))\n",
    "        \n",
    "        # =====================================================\n",
    "        #            Encoders and Decoding Generators update\n",
    "        # =====================================================\n",
    "\n",
    "        # Resetting gradients\n",
    "        optim_E.zero_grad()\n",
    "        optim_G.zero_grad()   \n",
    "\n",
    "        # Forward pass for B to A\n",
    "        latent_mel_B, mu_B, logvar_B = E(real_mel_B)\n",
    "        fake_mel_A = G_B2A(latent_mel_B)\n",
    "        fake_output_A = torch.squeeze(D_A(fake_mel_A))\n",
    "        \n",
    "        # Cyclic reconstuction from fake A to B\n",
    "        latent_fake_A, mu_fake_A, logvar_fake_A = E(fake_mel_A)\n",
    "        recon_mel_B = G_A2B(latent_fake_A)  \n",
    "        \n",
    "        # Forward pass for A to B\n",
    "        latent_mel_A, mu_A, logvar_A = E(real_mel_A)\n",
    "        fake_mel_B = G_A2B(latent_mel_A)\n",
    "        fake_output_B = torch.squeeze(D_B(fake_mel_B))\n",
    "        \n",
    "        # Cyclic reconstuction from fake B to A\n",
    "        latent_fake_B, mu_fake_B, logvar_fake_B = E(fake_mel_B)\n",
    "        recon_mel_A = G_B2A(latent_fake_B)  \n",
    "        \n",
    "        # Encoding loss A and B\n",
    "        kld_A = -0.5 * torch.sum(1 + logvar_A - mu_A.pow(2) - logvar_A.exp())\n",
    "        mse_A = (recon_mel_A - real_mel_A).pow(2).mean()\n",
    "        loss_VAE_A = kld_A + mse_A\n",
    "        \n",
    "        kld_B = -0.5 * torch.sum(1 + logvar_B - mu_B.pow(2) - logvar_B.exp())\n",
    "        mse_B = (recon_mel_B - real_mel_B).pow(2).mean()\n",
    "        loss_VAE_B = kld_B + mse_B\n",
    "        \n",
    "        errVAE = (loss_VAE_A + loss_VAE_B) / 2\n",
    "        errVAE.backward(retain_graph=True)  # retain graph so other losses can update in same graph\n",
    "        optim_E.step()\n",
    "        \n",
    "        # GAN loss\n",
    "        loss_GAN_B2A = loss_adversarial(fake_output_A, real_label)\n",
    "        loss_GAN_A2B = loss_adversarial(fake_output_B, real_label)\n",
    "        \n",
    "        # Cyclic loss\n",
    "        loss_cycle_ABA = loss_cycle(recon_mel_A, real_mel_A) * lambda_cycle\n",
    "        loss_cycle_BAB = loss_cycle(recon_mel_B, real_mel_B) * lambda_cycle\n",
    "        \n",
    "        # Backward pass and update all  \n",
    "        errG = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n",
    "        errG.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        \n",
    "        # =====================================================\n",
    "        #                   Discriminators update\n",
    "        # =====================================================\n",
    "        \n",
    "        # Resetting gradients\n",
    "        optim_D_A.zero_grad()\n",
    "        optim_D_B.zero_grad()\n",
    "        \n",
    "        # Forward pass D_A\n",
    "        real_out_A = torch.squeeze(D_A(real_mel_A))\n",
    "        fake_mel_A = fake_A_buffer.push_and_pop(fake_mel_A)\n",
    "        fake_out_A = torch.squeeze(D_A(fake_mel_A.detach()))\n",
    "        \n",
    "        loss_D_real_A = loss_adversarial(real_out_A, real_label)\n",
    "        loss_D_fake_A = loss_adversarial(fake_out_A, fake_label)\n",
    "        errD_A = (loss_D_real_A + loss_D_fake_A) / 2\n",
    "        \n",
    "        # Forward pass D_B\n",
    "        real_out_B = torch.squeeze(D_B(real_mel_B))\n",
    "        fake_mel_B = fake_B_buffer.push_and_pop(fake_mel_B)\n",
    "        fake_out_B = torch.squeeze(D_B(fake_mel_B.detach()))\n",
    "\n",
    "        loss_D_real_B = loss_adversarial(real_out_B, real_label)\n",
    "        loss_D_fake_B = loss_adversarial(fake_out_B, fake_label)\n",
    "        errD_B = (loss_D_real_B + loss_D_fake_B) / 2\n",
    "        \n",
    "        # Backward pass and update all\n",
    "        errD_A.backward()\n",
    "        errD_B.backward()\n",
    "        optim_D_A.step()\n",
    "        optim_D_B.step() \n",
    "        \n",
    "        # Update error log\n",
    "        pbar.set_postfix(errVAE=errVAE.item(), errG=errG.item(), errD_A=errD_A.item(), errD_B=errD_B.item())\n",
    "        \n",
    "    # Update error epoch history    \n",
    "    train_hist['VAE_A'].append(loss_VAE_A.item())\n",
    "    train_hist['VAE_B'].append(loss_VAE_B.item())\n",
    "    \n",
    "    train_hist['G_B2A'].append(loss_GAN_B2A)\n",
    "    train_hist['G_A2B'].append(loss_GAN_A2B)\n",
    "    train_hist['G_ABA'].append(loss_cycle_ABA)\n",
    "    train_hist['G_BAB'].append(loss_cycle_BAB)\n",
    "    train_hist['G'].append(errG.item())\n",
    "    \n",
    "    train_hist['D_A'].append(errD_A.item())\n",
    "    train_hist['D_B'].append(errD_B.item())    \n",
    "    \n",
    "    \n",
    "# Saving\n",
    "save_pickle(train_hist, 'pool/01/train_hist.pickle')\n",
    "torch.save(G.state_dict(), 'pool/01/G.pt')\n",
    "torch.save(E.state_dict(), 'pool/01/E.pt')\n",
    "torch.save(D_A.state_dict(), 'pool/01/D_A.pt')\n",
    "torch.save(D_B.state_dict(), 'pool/01/D_B.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-warren",
   "metadata": {},
   "source": [
    "### Plotting generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_buffer(buffer):\n",
    "    for d in buffer.data:\n",
    "        mel = torch.squeeze(d).cpu().numpy()\n",
    "        show_mel(mel)\n",
    "        \n",
    "#plot_buffer(fake_A_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-leisure",
   "metadata": {},
   "source": [
    "## Plotting errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_train_hist(train_hist, names, title):\n",
    "    x = range(len(train_hist[names[0]]))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    for n in names:\n",
    "        plt.plot(x, train_hist[n], label=n)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "# Loading history\n",
    "train_hist = load_pickle('pool/train_hist.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE losses\n",
    "plot_train_hist(train_hist, ['VAE_A', 'VAE_B'], 'Variational Auto Encoder Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Generator losses\n",
    "plot_train_hist(train_hist, ['G_B2A', 'G_A2B'], 'Adversarial Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclic losses\n",
    "plot_train_hist(train_hist, ['G_ABA', 'G_BAB'], 'Cyclic Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminators loss\n",
    "plot_train_hist(train_hist, ['D_B', 'D_A'], 'Discriminator Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
